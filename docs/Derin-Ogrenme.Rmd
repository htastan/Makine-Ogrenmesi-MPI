---
title: "Derin Ã–ÄŸrenme"
author: "HÃ¼seyin TaÅŸtan"
subtitle: Makine Ã–ÄŸrenmesi (MP Ä°ktisat TYL)
institute: YÄ±ldÄ±z Teknik Ãœniversitesi
# date: " "
output:
  xaringan::moon_reader:
    self_contained: true
    css: [default, metropolis]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
  html_document:
    df_print: paged 
---

<style type="text/css">
.remark-slide-content {
    font-size: 25px;
    padding: 1em 4em 1em 4em;
}
.my-large-font {
  font-size: 40px;
}
.my-small-font {
  font-size: 20px;
}
.my-medium-font {
  font-size: 25px;
}
</style>



# Plan

 
- [Yapay Sinir AÄŸlarÄ±](#ysa) 

- [KÄ±sa tarihÃ§e](#history)

- [ANN'lerin yapÄ±sÄ±](#struct)

- [Basit bir Ã¶rnek](#ex1)

- [Ã‡ok katmanlÄ± aÄŸlar](#multi)

- [Sinir aÄŸlarÄ±nÄ±n eÄŸitilmesi](#training)

- [Convolutional Neural Networks](#cnn)

- [Recurrent Neural Networks](#rnn)

- [Metin verileri](#text)


---
name: ysa 

# Yapay Sinir AÄŸlarÄ± (YSA) 

![:scale 200%](img/ann1.PNG)  
 
- Yapay sinir aÄŸlarÄ±, beyindeki sinir hÃ¼crelerinin (nÃ¶ronlarÄ±n) yapÄ±sÄ± ve iÅŸlevinden esinlenen, doÄŸrusal olmayan ve oldukÃ§a esnek modellerdir.
- Biyolojik sinir aÄŸlarÄ±, elektrik sinyalleri ile iletiÅŸim kuran nÃ¶ronlardan oluÅŸur ve birlikte karmaÅŸÄ±k Ã§Ä±ktÄ±lar Ã¼retirler.
- Her nÃ¶ron, gelen sinyalleri toplar ve bu sinyallerin toplam gÃ¼cÃ¼ belirli bir eÅŸik deÄŸeri aÅŸarsa â€œaktifleÅŸirâ€ ve baÄŸlÄ± nÃ¶ronlara elektriksel bir sinyal gÃ¶nderir.
- Birbirine baÄŸlÄ± bu nÃ¶ron aÄŸÄ±, beyinde karmaÅŸÄ±k bilgi iÅŸlemi, Ã¶ÄŸrenme ve karar verme sÃ¼reÃ§lerini mÃ¼mkÃ¼n kÄ±lar.

---
# Yapay Sinir AÄŸlarÄ± (YSA) 

![:scale 200%](img/ann1.PNG)  
 
- Benzer ÅŸekilde, bir YSA modeli, Ã§ok sayÄ±da girdiyi bilinmeyen bir parametre vektÃ¶rÃ¼ ile aÄŸÄ±rlÄ±klandÄ±rarak Ã§Ä±ktÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

-  Ã‡Ä±ktÄ± deÄŸiÅŸkeni birden fazla olabilir. 

- Hem regresyon hem de sÄ±nÄ±flandÄ±rma problemlerine uygulanabilir.   

---
name: history

# Derin Ã–ÄŸrenmenin Tarihi


- 1940 - 1950 
  - Yapay sinir aÄŸlarÄ± ilk kez beyindeki biyolojik sinir aÄŸlarÄ±ndan esinlenerek tanÄ±tÄ±ldÄ±.

- 1960 - 1970 
  - Ã–zellikle Frank Rosenblatt tarafÄ±ndan perceptronlarÄ±n geliÅŸtirilmesiyle yapay sinir aÄŸlarÄ± alanÄ±nda Ã¶nemli ilerlemeler kaydedildi.
  - Ancak, perceptronlarÄ±n doÄŸrusal olmayan problemleri Ã§Ã¶zmedeki sÄ±nÄ±rlÄ±lÄ±klarÄ±, yapay sinir aÄŸlarÄ±na olan ilginin azalmasÄ±na neden oldu.

---
# KÄ±sa tarihÃ§e

- 1980 - 1990 
  - Geriye yayÄ±lÄ±m (backpropagation) algoritmasÄ± yeniden keÅŸfedildi ve popÃ¼ler hale getirildi. Ã‡ok katmanlÄ± perceptronlarÄ±n verimli bir ÅŸekilde eÄŸitilmesi olanaklÄ± hale geldi. 
  - Bu ilerlemelere raÄŸmen, yapay sinir aÄŸlarÄ± karmaÅŸÄ±k gÃ¶revleri iÅŸlemede zorluklarla karÅŸÄ±laÅŸtÄ± ve diÄŸer makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±nÄ±n gÃ¶lgesinde kaldÄ±.


- 2000 - 2010 
  - Yapay sinir aÄŸlarÄ±na olan ilginin yeniden canlanmasÄ±, derin Ã¶ÄŸrenmedeki Ã§Ä±ÄŸÄ±r aÃ§an geliÅŸmelerle baÅŸladÄ±. Yann LeCun ve diÄŸerleri tarafÄ±ndan gÃ¶rÃ¼ntÃ¼ tanÄ±ma problemlerinde baÅŸarÄ±lÄ± sonuÃ§lar veren EvriÅŸimli (Convolutional) Sinir AÄŸlarÄ± (CNN'ler) tanÄ±tÄ±ldÄ±.
  - ArdÄ±ÅŸÄ±k veri iÅŸleme iÃ§in Tekrarlayan (Recurrent) Sinir AÄŸlarÄ± (RNN'ler), konuÅŸma tanÄ±ma ve doÄŸal dil iÅŸleme alanlarÄ±nda ilerlemelere yol aÃ§tÄ±.
  
---
# Derin Ã–ÄŸrenmenin Tarihi

  
- 2012 -  gÃ¼nÃ¼mÃ¼z
  - 2012'de ImageNet BÃ¼yÃ¼k Ã–lÃ§ekli GÃ¶rsel TanÄ±ma YarÄ±ÅŸmasÄ± (ILSVRC), AlexNet'in zaferiyle Ã¶nemli bir aÅŸama kaydetti. Bkz.  <https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge>
 
- O zamandan beri, derin Ã¶ÄŸrenme, nesne tespiti, makine Ã§evirisi ve otonom sÃ¼rÃ¼ÅŸ gibi gÃ¶revlerde dikkate deÄŸer baÅŸarÄ±lar elde ederek Ã§eÅŸitli alanlarda baskÄ±n bir yaklaÅŸÄ±m haline gelmiÅŸtir.
  
- DonanÄ±m (Ã¶rneÄŸin GPU'lar) ve yazÄ±lÄ±m (TensorFlow ve PyTorch gibi derin Ã¶ÄŸrenme Ã§erÃ§eveleri) alanÄ±ndaki geliÅŸmeler, derin Ã¶ÄŸrenmenin ilerlemesini daha da hÄ±zlandÄ±rmÄ±ÅŸtÄ±r.
 
---
name: struct

# YSA'nÄ±n yapÄ±sÄ±


.pull-left[
![](img/ann3.PNG) 
- Yapay sinir aÄŸlarÄ±nÄ±n yapÄ±sÄ± bir ÅŸebeke grafiÄŸi ile stilize bir ÅŸekilde gÃ¶sterilebilir. (image source: James et al. ISLR, 2nd ed., p.405) 

]
--
.pull-right[
- Tipik bir YSA Ã¼Ã§ ana katmandan oluÅŸur: girdi katmanÄ± (input), gizli katmanlar (hidden layers), ve Ã§Ä±ktÄ± katmanÄ± (output)

- GiriÅŸ katmanÄ±, girdi (kestirim) verilerini alÄ±r ve Ã§Ä±kÄ±ÅŸ katmanÄ±na ulaÅŸmadan Ã¶nce gizli katmanlar aracÄ±lÄ±ÄŸÄ±yla iÅŸlenir.

- Bu aÄŸ mimarisi, YSA'nÄ±n regresyon ve sÄ±nÄ±flandÄ±rma dahil olmak Ã¼zere Ã§eÅŸitli gÃ¶revlerde etkili bir ÅŸekilde Ã¶ÄŸrenmesine ve tahmin yapmasÄ±na olanak tanÄ±r.

]

---
# YSA (ANN) 

- ANN Ã§ok katmanlÄ± veya tek katmanlÄ± olabilir.
- Bir ANN'nin bileÅŸenleri: 
  - Girdiler: $x_1, x_2,\ldots,x_p$ 
  - AÄŸÄ±rlÄ±klar (bilinmeyen parametreler): $w_1,w_2,\ldots,w_p$, ve $w_0$ (sabit ya da sapma/bias) 
  - Toplama fonksiyonu: $\sum_{j=1}^{p} w_{j} x_{j}+w_{0}$ 
  - Aktivasyon fonksiyonu: $g(\cdot)$ 
- Basit bir ANN (perceptron/algÄ±layÄ±cÄ±) ÅŸu ÅŸekilde yazÄ±labilir: 
$$y=g\left( \sum_{j=1}^{d} w_{j} x_{j}+w_{0} \right)$$

---
# ğŸ”§ Tek Gizli KatmanlÄ± bir ANN'nin BileÅŸenleri

Gizli katmandaki birim (neuron) sayÄ±sÄ± $K$ olsun. Modelin katmanlarÄ±:

1. **Gizli katman (girdilerin doÄŸrusal olmayan transformasyonu)**  
   Her bir gizli katman iÃ§in $k = 1, \ldots, K$:
   $$A_k = h_k(X) = g\left(w_{k0} + \sum_{j=1}^{p} w_{kj} X_j \right)$$
where 
   - $g(\cdot)$: aktivasyon fonksiyonu (e.g., ReLU, sigmoid)  
   - $w_{kj}$: $j$ girdisinden $k$ birimine aÄŸÄ±rlÄ±klar  
   - $w_{k0}$: $k$ birimi iÃ§in sapma/bias terimi 

---
# ğŸ§¾ Ã‡Ä±ktÄ± katmanÄ±

Ã‡Ä±ktÄ±, gizli birimlerin  doÄŸrusal bir fonksiyonudur:
$$f(X) = \beta_0 + \sum_{k=1}^{K} \beta_k A_k$$

- Aktivasyonlar $A_k$, dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ Ã¶zellikler gibi davranÄ±r.  

- KatsayÄ±lar $\beta_0, \ldots, \beta_K$ ile aÄŸÄ±rlÄ±klar $w_{kj}$, veriden tahmin edilir.  

- AÄŸ, girdileri doÄŸrusal olmayan aktivasyon fonksiyonlarÄ±yla dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, ardÄ±ndan Ã§Ä±ktÄ± katmanÄ±nda doÄŸrusal bir kombinasyon yapar.  

- Ayarlanma parametreleri: Modelin yapÄ±sÄ±nÄ± ve Ã¶ÄŸrenme sÃ¼recini kontrol eden Ã§eÅŸitli parametreler. Bu parametreler eÄŸitim sÄ±rasÄ±nda performansÄ± etkiler ve genellikle deneysel olarak seÃ§ilir


---
# Aktivasyon FonksiyonlarÄ±

![](img/activation.PNG) 
- Aktivasyon fonksiyonlarÄ± 
([Source](https://en.wikipedia.org/wiki/Activation_function))

- YaygÄ±n kullanÄ±lanlar: Sigmoid ve Rectified Linear (ReLu) 


---
# Sigmoid Aktivasyon fonksiyonu

.pull-left[ 
![](img/sigmoid.PNG) 
- Sigmoid aktivasyon fonksiyonu (kÄ±rmÄ±zÄ±): 
$$\sigma(v) = \frac{1}{1+e^{-v}}$$

]
--
.pull-right[
- Grafikte ayrÄ±ca $\sigma(sv)$ iki deÄŸer iÃ§in gÃ¶sterilmiÅŸtir: $s=0.5$ (mavi), 
$s=10$ (mor) (Kaynak: Elements of Statistical Learning, p.394)
- Aktivasyon fonksiyonu $\sigma=1$ olduÄŸunda (identity function) model girdilere gÃ¶re doÄŸrusal olur. 
- Bir YSA aslÄ±nda doÄŸrusal sÄ±nÄ±flandÄ±rma ya da regresyon problemlerinin 
doÄŸrusal olmayan genelleÅŸtirmesi olarak dÃ¼ÅŸÃ¼nÃ¼lebilir. 
]

---
# ReLU aktivasyon fonksiyonu

.pull-left[ 
![](img/relu.PNG) 
- Rectified Linear Unit (ReLU) aktivasyon fonksiyonu: 
$$g(z)=(z)_{+}= \begin{cases}0 & \text { if } z<0\\z & \text { otherwise }\end{cases}$$

]
--
.pull-right[
- $z$'nin negatif deÄŸerleri iÃ§in aktivasyon fonksiyonu 0 olur. 
- DiÄŸer deÄŸerler iÃ§in $z$ olur. 
- ReLU fonksiyonu sigmoid fonksiyonundan daha verimli bir ÅŸekilde saklanabilir
- Detaylar iÃ§in bkz. ISLR2, p. 405. 
]

---
name: ex1 

# Ã–rnek: Basit bir ileri beslemeli YSA  


$$\hat{y} = -3.01408(-2.46247 - 0.76578x_1+1.49965x_2-1.29388x_3) + 2.17061$$
$$\hat{y}\approx 9.593+2.308x_1-4.52x_2+3.90x_3$$
.pull-left[ 
![:scale 90%](img/nnet0.PNG) 
]
--
.pull-right[ 
- 3 girdi, tek katman, aktivasyon fonksiyonu: $h(z)=z$ (identity)
- AslÄ±nda bu OLS regresyonu ile aynÄ±dÄ±r: 
![](img/nnet0reg.PNG) 
- Bir yapay sinir aÄŸacÄ± $x$ deÄŸiÅŸkenleri arasÄ±ndaki etkileÅŸimi aÃ§Ä±k olarak belirtmeden yakalayabilir.
]


---
# Ã–rnek
 
.pull-left[ 
![](img/nnet1.PNG) 
]
--
.pull-right[ 
- Tek gizli katmanlÄ±, Ã¼Ã§ hÃ¼creli bir ileri beslemeli yapay sinir aÄŸÄ± (mavi renkte
gÃ¶sterilenler sabitlerdir; YSA jargonunda "bias")
- Bu model deÄŸiÅŸken etkileÅŸimlerine izin verir. 
]


---
# MNIST handwritten-digit problem 

.pull-left[ 
![](img/mnist.PNG) 
- Bu problem, el yazÄ±sÄ± rakamlarÄ± doÄŸru bir ÅŸekilde tanÄ±mlamak iÃ§in bir sÄ±nÄ±flandÄ±rma problemidir, $0,1,\ldots,9$
(kaynak: James et al. ISLR, 2nd ed., p.407) 
]
--
.pull-right[ 
- El yazÄ±sÄ± rakamlar, $28\times 28$ gri tonlamalÄ± gÃ¶rÃ¼ntÃ¼ler olarak kaydedilir.

- EÄŸitim setinde 60000 gÃ¶rÃ¼ntÃ¼, test setinde ise 10000 gÃ¶rÃ¼ntÃ¼ bulunmaktadÄ±r. 

- Ã–zellikler: 784 piksel gri tonlama deÄŸerleri

- Etiketler: rakamlar $0,1,\ldots,9$

- Ä°nsan hata oranÄ±: %0.2 (10000'de 20)

- Makine hata oranlarÄ±: en iyi oranlar %0.5'in altÄ±ndadÄ±r.
]

---
name: multi

# Ã‡ok katmanlÄ± aÄŸlar


.pull-left[
![](img/ann4.PNG) 
- Burada $L_1$ and $L_2$ ile gÃ¶sterilen 2 gizli katman ve Ã§oklu Ã§Ä±ktÄ±lar bulunmaktadÄ±r. (kaynak: James et al. ISLR, 2nd ed., p.409) 

]
--
.pull-right[
- Bu Ã¶rnek, el yazÄ±sÄ± rakam tanÄ±ma (MNIST problemi) iÃ§in uygundur.

- Girdi katmanÄ±nda 784 birim bulunurken, iki gizli katmanda sÄ±rasÄ±yla $K_1=256$ ve $K_2=128$ birim (hÃ¼cre) bulunmaktadÄ±r. Ã‡Ä±ktÄ± katmanÄ±nda ise 10 birim (rakamlar $0,1,\ldots,9$) bulunmaktadÄ±r. 

- Bu YSA'da sabitler (sapma) dahil olmak Ã¼zere 235146 parametre (aÄŸÄ±rlÄ±k) vardÄ±r.  


]

---
name: training

# Yapay Sinir AÄŸlarÄ±nÄ±n EÄŸitilmesi

- YSA modellerinde amaÃ§ fonksiyonu regresyon problemleri iÃ§in kalÄ±ntÄ± kareleri toplamÄ±: 

$$R(\theta)=\sum_{k=1}^{K} \sum_{i=1}^{N}\left(y_{i k}-f_{k}\left(x_{i}\right)\right)^{2}$$

- SÄ±nÄ±flandÄ±rma problemleri iÃ§in ise hata karesi ya da Ã§apraz-entropi (deviance):  
$$R(\theta)=-\sum_{i=1}^{N} \sum_{k=1}^{K} y_{i k} \log f_{k}\left(x_{i}\right)$$

$\theta$: bilinmeyen aÄŸÄ±rlÄ±k vektÃ¶rÃ¼, $y_{i k}$: gÃ¶zlem deÄŸerleri, $f_{k}$: $k$ Ã§Ä±ktÄ±sÄ± iÃ§in tahmin deÄŸeri, $K$ Ã§Ä±ktÄ± sayÄ±sÄ±nÄ±, $N$ gÃ¶zlem sayÄ±sÄ± 

---
# Yapay Sinir AÄŸlarÄ±nÄ±n EÄŸitilmesi

- Minimum $R(\theta)$ deÄŸerini veren aÄŸÄ±rlÄ±klar gradyan iniÅŸ algoritmasÄ± ile bulunabilir (gradient descent). 

- Bu algoritma geri-yayÄ±lÄ±m algoritmasÄ± olarak da bilinir (back-propagation algorithm). 

- Gradyan (birinci tÃ¼rev vektÃ¶rÃ¼)  zincir kuralÄ± ile kolayca hesaplanabilir. 

- Fazla uyumdan kaÃ§Ä±nmak iÃ§in genellikle gradyan vektÃ¶rÃ¼ kÃ¼Ã§Ã¼k pozitif bir parametre ile Ã§arpÄ±lÄ±r. BÃ¶ylece Ã¶ÄŸrenme hÄ±zÄ± kontrol edilebilir. 

- AlgoritmanÄ±n detaylarÄ± iÃ§in bkz. ss. 395-397, Hastie, Tibshirani, Friedman (2017), Elements of Statistical Learning, 2nd ed. Springer 


---
# Gradyan iniÅŸ algoritmasÄ±

![](img/gdescent.PNG) 

(kaynak: James et al. ISLR, 2nd ed., p.435) 
---
name: cnn

# EvriÅŸimli (Convolutional) Yapay Sinir AÄŸlarÄ± (CNNs)
.pull-left[
![](img/cnn0.PNG) 

(kaynak: James et al. ISLR, 2nd ed., p.412) 
]
--
.pull-right[
- CNN'ler, Ã¶zellikle gÃ¶rÃ¼ntÃ¼lerin sÄ±nÄ±flandÄ±rÄ±lmasÄ± iÃ§in oldukÃ§a yararlÄ± olabilir.
- Bu Ã¶rnekte bir kaplan resminin kÃ¶ÅŸe, renk parÃ§alarÄ± gibi daha yerel, dÃ¼ÅŸÃ¼k dÃ¼zeydeki Ã¶zellikleri belirlenir. Daha sonra bu yerel Ã¶zellikler birleÅŸtirilerek daha yÃ¼ksek dÃ¼zeydeki Ã¶zellikler belirlenir (gÃ¶z, kulak, vb). Bu birleÅŸtirilmiÅŸ Ã¶zellikler resmin sÄ±nÄ±flandÄ±rÄ±lmasÄ±nda kullanÄ±lÄ±r. 
- CNN, bir gÃ¶rÃ¼ntÃ¼yÃ¼ hiyerarÅŸik bir ÅŸekilde oluÅŸturur. 
]

---
# EvriÅŸimli (Convolutional) Yapay Sinir AÄŸlarÄ± (CNNs)
.pull-left[
![](img/cnn1.PNG) 

(kaynak: James et al. ISLR, 2nd ed., p.414) 
]
--
.pull-right[
- Bu hiyerarÅŸik yapÄ±nÄ±n oluÅŸturulmasÄ±, evriÅŸim (convolution) ve havuzlama (pooling) katmanlarÄ± kullanÄ±larak gerÃ§ekleÅŸtirilir. 
- Convolution katmanÄ± Ã§ok sayÄ±da filtreden oluÅŸur. Bu filtreler yerel Ã¶zelliklerin belirlenmesinde kullanÄ±lÄ±r. 
- Pooling (havuzlama) katmanÄ± ise Ã§ok sayÄ±da gÃ¶rÃ¼ntÃ¼yÃ¼ Ã¶zet bir gÃ¶rÃ¼ntÃ¼ye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. ]
---
# CNNs 

.pull-left[
![](img/cnn1.PNG) 

(kaynak: James et al. ISLR, 2nd ed., p.414) 
]
--
.pull-right[
- Filtrenin kendisi de bir gÃ¶rÃ¼ntÃ¼dÃ¼r ve kÃ¼Ã§Ã¼k bir ÅŸekil, kenar vb. temsil eder. Bu filtreyi girdi gÃ¶rÃ¼ntÃ¼sÃ¼ etrafÄ±nda kaydÄ±rÄ±r ve eÅŸleÅŸmeler iÃ§in puan verilir. 
- Girdi gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n alt gÃ¶rÃ¼ntÃ¼sÃ¼, filtreye benziyorsa, puan yÃ¼ksek olur, aksi takdirde dÃ¼ÅŸÃ¼ktÃ¼r.
- Ã–rnekteki kaplan resminde iki kÃ¼Ã§Ã¼k evriÅŸim filtresi uygulanmÄ±ÅŸtÄ±r. Ãœstteki evriÅŸimli gÃ¶rÃ¼ntÃ¼ kaplanÄ±n dikey Ã§izgilerini, alttaki ise yatay Ã§izgilerini vurgular.

- Filtreler eÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrenilir.  
]
---
name: rnn

# Tekrarlayan (Recurrent) Sinir AÄŸlarÄ± (RNNs)

![](img/rnn1.PNG) 

- RNN'ler, zaman serileri, kaydedilmiÅŸ konuÅŸma, mÃ¼zik veya belgedeki kelimelerin bir dizisi gibi sÄ±ralÄ± veriler iÃ§in Ã¶zellikle yararlÄ±dÄ±r.

- RNN'ler, verinin bu ardÄ±ÅŸÄ±k doÄŸasÄ±nÄ± dikkate alarak modeller oluÅŸturur ve geÃ§miÅŸi belleÄŸinde saklar.



(kaynak: James et al. ISLR, 2nd ed., p.422) 

---
# Tekrarlayan (Recurrent) Sinir AÄŸlarÄ± (RNNs)


- RNN'lerde Ã¶zellik kÃ¼mesi, vektÃ¶rlerin bir dizisi olarak temsil edilir $X=\{X_1,X_2,\ldots,X_L\}$. Ã‡Ä±ktÄ± deÄŸiÅŸkeni $Y$  de bir vektÃ¶r olabilir (Ã¶rneÄŸin aynÄ± belgenin baÅŸka bir dildeki tercÃ¼mesi). 

- Bu Ã¶rnekte tek hedef deÄŸiÅŸkeni $Y$ bulunmaktadÄ±r. 

- RNN girdileri $\{X_1,X_2,\ldots,X_L\}$ ardÄ±ÅŸÄ±k olarak iÅŸler.  Her $X_l$ gizli katmana mevcut aktivasyon vektÃ¶rÃ¼ $A_l$ ile beslenir. 

- $A_l$, girdi olarak hem $X_l$ hem de $A_{l-1}$'i alÄ±r ve bir Ã§Ä±ktÄ± $O_l$ Ã¼retir. 

- $W$, $U$, ve $B$ aÄŸÄ±rlÄ±klardÄ±r 

- Tekrarlayan (recurrent) = aynÄ± aÄŸÄ±rlÄ±klarÄ±n sÄ±ralÄ± adÄ±mlarda kullanÄ±lmasÄ±.

- Matematiksel detaylar iÃ§in ders kitabÄ±nÄ±n 423-4. sayfalarÄ±na bakabilirsiniz.

---
# LSTM 

- Long Term and Short Term Memory (LSTM) ya da Uzun Vadeli ve KÄ±sa Vadeli Bellek, RNN'lerin bir varyasyonudur.  

- Ekonomik ve finansal zaman serileri verilerinde, mevcut deÄŸer hem mevcut ÅŸoklara hem de geÃ§miÅŸ ÅŸoklara baÄŸlÄ± olabilir.

- LSTM RNN'ler, hem mevcut deÄŸerlerden (kÄ±sa vadeli) hem de uzak geÃ§miÅŸten gelen deÄŸerlerden aktivasyonlarÄ± korur. Bu nedenle, LSTM RNN'ler, erken sinyallerin zamanla silinmesi sorununu aÅŸar, Ã§Ã¼nkÃ¼ bu sinyaller zincir boyunca son aktivasyon vektÃ¶rÃ¼ne iletilene kadar korunur (ISLR2, s. 426).

- Bu Ã¶zellik, finans ve ekonomide tahmin yaparken Ã¶zellikle yararlÄ± olabilir Ã§Ã¼nkÃ¼ zaman serisi verileri genellikle yÃ¼ksek oranda sÃ¼rekli (otokorelasyonlu) olabilir.

(NYSE Ã¶rneÄŸi iÃ§in 427. sayfaya bakÄ±nÄ±z)

---
# NYSE Ã–rneÄŸi (sf. 427)

.pull-left[
![:scale 90%](img/lstm1.PNG) 

(GÃ¶rsel kaynaÄŸÄ±: James ve ark., ISLR, 2. baskÄ±, sf. 427) 
]

.pull-right[
- Bu ÅŸekil, log iÅŸlem hacmini, Dow Jones gÃ¼nlÃ¼k getirilerini ve log volatiliteyi (gÃ¼nlÃ¼k aralÄ±k mutlak deÄŸeri) gÃ¶steriyor.  
- Hedef: log iÅŸlem hacmini tahmin etmek.  
- Veri: Ã¶nemli otokorelasyona sahip zaman serisi â€” geÃ§miÅŸ deÄŸerler ÅŸimdiki deÄŸerlerle gÃ¼Ã§lÃ¼ ÅŸekilde iliÅŸkili.  
- RNN yaklaÅŸÄ±mÄ±: zaman baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± modellemek iÃ§in deÄŸiÅŸkenlerin gecikmeli deÄŸerlerini sÄ±ralÄ± girdiler olarak dahil etmek.  
- RNN avantajÄ±: geleneksel modellerin gÃ¶zden kaÃ§Ä±rabileceÄŸi karmaÅŸÄ±k Ã¶rÃ¼ntÃ¼leri ve doÄŸrusal olmayan dinamikleri yakalar.  
]

<!-- --- -->

<!-- ![:scale 50%](img/lstm1.PNG)  -->

<!-- (image source: James et al. ISLR, 2nd ed., p.427)  -->

---
name: text 

# Metin/Belge SÄ±nÄ±flandÄ±rmasÄ±

- Metin verisi, mÃ¼ÅŸteri yorumlarÄ±, geri bildirimler, ekonomik raporlar, haber makaleleri ve sosyal medya paylaÅŸÄ±mlarÄ± gibi ekonomik bilgi iÃ§in faydalÄ± kaynaklarÄ± iÃ§erir.

- Genellikle **yapÄ±landÄ±rÄ±lmamÄ±ÅŸtÄ±r**: kelime dizileri veya koleksiyonlarÄ± olarak temsil edilir, sabit sayÄ±sal forma sahip deÄŸildir.

- Geleneksel YaklaÅŸÄ±m: **Kelime TorbasÄ± (BoW - Bag-of-Words)** â€” kelime sÄ±rasÄ±nÄ± gÃ¶z ardÄ± ederek metni kelime frekans vektÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.


- BoW ile Duygu Analizi:
  - Kelimelere duygu skorlarÄ± atar (pozitif, negatif, nÃ¶tr).
  - Kelime skorlarÄ±nÄ± toplayarak belgenin ya da yorumun genel duygusunu Ã§Ä±karÄ±r.
  - Metin verilerinden tÃ¼ketici gÃ¼veni veya piyasa havasÄ± Ã¶lÃ§mekte kullanÄ±labilir.

- BoW sÄ±nÄ±rlamalarÄ±: anlam iÃ§in Ã¶nemli olan baÄŸlamÄ± ve kelime sÄ±ralamasÄ±nÄ± kaybeder.

---
# Metin/Belge Verisi iÃ§in Derin Ã–ÄŸrenme

- Derin Ã¶ÄŸrenme, anlamsal bilgiyi yoÄŸun vektÃ¶r biÃ§iminde yakalayan kelime gÃ¶mme (Word2Vec, GloVe) yÃ¶ntemlerini kullanÄ±r.

- RNN, LSTM ve Transformer (Ã¶rneÄŸin BERT) gibi yÃ¶ntemler kelime sÄ±rasÄ±nÄ± ve baÄŸlamÄ± yakalar.

- Bu yÃ¶ntemler duygu analizi, konu modelleme ve belge sÄ±nÄ±flandÄ±rma gibi gÃ¶revlerde Ã¶nemli geliÅŸmeler saÄŸlar.

- Ekonomi alanÄ±nda uygulamalar:
  - Finans haberleri ve raporlarÄ±nda piyasa duyarlÄ±lÄ±ÄŸÄ±nÄ± analiz etmek.
  - Ekonomik gÃ¶stergelerin tahmini iÃ§in metinsel verilerden Ã¶zellik Ã§Ä±karmak.
  - Ekonomik trendlerle ilgili politika belgeleri veya sosyal medyadaki Ã¶rÃ¼ntÃ¼leri tespit etmek.

---
# Duygu Analizi iÃ§in RNN KullanÄ±mÄ±

- Problem: Film yorumlarÄ±nÄ± pozitif ya da negatif olarak sÄ±nÄ±flandÄ±rmak.
- Veri: Her yorum bilinen bir duygu etiketiyle eÅŸleÅŸir (pozitif veya negatif) â€” denetimli Ã¶ÄŸrenme.
- Girdi: Yorumdan bir kelime dizisi (Ã¶rneÄŸin "Film mÃ¼thiÅŸ ve heyecan vericiydi").

- RNN nasÄ±l Ã§alÄ±ÅŸÄ±r:
  - Kelimeleri tek tek okur, baÄŸlamla ilgili â€œhafÄ±zasÄ±nÄ±â€ gÃ¼nceller.
  - Ä°lk kelimelerin sonraki kelimelerin anlamÄ±nÄ± nasÄ±l etkilediÄŸini yakalar (Ã¶rn. "kÃ¶tÃ¼ deÄŸil" ile "kÃ¶tÃ¼" arasÄ±ndaki fark).
- EÄŸitim: Model, tahmin ettiÄŸi duygu ile gerÃ§ek etiket arasÄ±ndaki hatayÄ± minimize edecek ÅŸekilde parametrelerini ayarlar.
- Ã‡Ä±ktÄ±: Yorumun tamamÄ± iÅŸlendiÄŸinde pozitif ya da negatif duygu tahmini yapar.
- Neden daha iyi: Basit BoWâ€™dan farklÄ± olarak RNN, kelime sÄ±rasÄ±nÄ± ve baÄŸlamÄ± anlar, bÃ¶ylece daha doÄŸru duygu sÄ±nÄ±flandÄ±rmasÄ± yapar.

---
# RNNâ€™lerin Metin Analizindeki SÄ±nÄ±rlamalarÄ±

- Metnin Ã§ok gerisindeki bilgileri hatÄ±rlamakta zorlanÄ±rlar:
  - CÃ¼mlenin ya da paragrafÄ±n Ã§ok Ã¶nceki kelime ve fikirlerini kullanmakta gÃ¼Ã§lÃ¼k Ã§ekerler.
- Kelimeleri teker teker iÅŸlerler:
  - Bu durum eÄŸitimi yavaÅŸlatÄ±r Ã§Ã¼nkÃ¼ birÃ§ok kelimeyi aynÄ± anda iÅŸleyemezler (paralel iÅŸlemeyi zorlaÅŸtÄ±rÄ±r).
- SÄ±nÄ±rlÄ± baÄŸlamÄ± gÃ¶z Ã¶nÃ¼nde tutabilirler:
  - GeliÅŸmiÅŸ versiyonlarÄ± bile Ã§ok uzun metinleri tam anlamÄ±yla kavramakta zorlanÄ±r.
- Ä°yi Ã¶ÄŸrenmek iÃ§in Ã§ok sayÄ±da etiketli Ã¶rnek gerekir:
  - Yeterli ve kaliteli veri toplamak maliyetli veya zor olabilir.
- KarmaÅŸÄ±k dil yapÄ±larÄ±yla ilgili zorluklar:
  - Ä°roni, ince anlam veya belirsiz cÃ¼mleleri anlamakta gÃ¼Ã§lÃ¼k Ã§ekerler.
- **Neden Ã¶nemli:** Bu zorluklar, birÃ§ok kelimeyi aynÄ± anda iÅŸleyebilen, uzun menzilli baÄŸlamÄ± daha iyi kavrayan ve gÃ¼nÃ¼mÃ¼zÃ¼n gÃ¼Ã§lÃ¼ **BÃ¼yÃ¼k Dil Modellerinin (LLMâ€™ler)** temelini oluÅŸturan **transformer** mimarilerinin geliÅŸtirilmesine ilham vermiÅŸtir.

