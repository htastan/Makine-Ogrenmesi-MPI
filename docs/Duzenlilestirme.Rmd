---
title: "Düzenlileştirme"
author: "Hüseyin Taştan"
# date: " "
output:
  xaringan::moon_reader:
    css: [default, metropolis]
    lib_dir: libs
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
  html_document:
    df_print: paged
  pdf_document: default
subtitle: Makine Öğrenmesi (MP İktisat TYL)
institute: Yıldız Teknik Üniversitesi
---
class: my-medium-font

<style type="text/css">
.remark-slide-content {
    font-size: 25px;
    padding: 1em 4em 1em 4em;
}
.my-large-font {
  font-size: 40px;
}
.my-small-font {
  font-size: 20px;
}
.my-medium-font {
  font-size: 30px;
}
</style>



# Plan

- Doğrusal Model Seçimi 

- Değişken altkümelerinin seçimi 

- Düzenlileştirme (Regularization) kavramı 

- Çıkıntı regresyonu (Ridge regression)

- LASSO 

- Elastik Net

- Double-selection LASSO 

- Boyut küçültme 

---
# Doğrusal model seçimi 

- Pratikte genellikle bir başlangıç noktası olan doğrusal modeller çeşitli şekillerde geliştirilebilir. 

- Bunlardan biri modelin doğrusal (additive) yapısını bozmadan doğrusal olmayan ilişkilerin eklenmesidir (örneğin polinom terimleri, splinelar, GAM). Modelin parametrelerde doğrusal yapısı aynı kaldığından en küçük kareler yöntemi uygulanabilir. 

- Diğer bir yaklaşım ilgisiz kestirim değişkenlerin dışlanarak modelin performansının iyileştirilmesidir. 

- Değişken sayısının, $p$, çok fazla olduğu durumlarda özellikle faydalı olabilir. Yaygın kullanılan yöntemler şunlardır: 
  - Altküme seçimi
  - Düzenlileştirme yaklaşımı (Shrinkage or regularization): bu yaklaşımda tüm değişkenleri dikkate alınır ancak bazılarının katsayıları sıfıra doğru küçültülür. 
  - Boyut küçültme. 

--- 
# Altküme seçimi 

- Burada amaç doğrusal modelde kestirim açısından en önemli/faydalı değişkenlerin belirlenmesidir.  

- En iyi altküme seçimi, ileriye ya da geriye doğru adım adım seçim  

- **En iyi altküme seçimi**: Olanaklı tüm altkümelerin tahmin edilmesini gerektirir. Bu nedenle $p$ büyükse pratikte uygulanamayabilir. 

- **İleriye doğru adım adım seçim**: hiç kestirim değişkeninin olmadığı boş (null) modelden başlanarak değişkenler performansı arttırıp arttırmadığına göre teker teker eklenir.  

- **Geriye doğru adım adım seçim**: bu yöntemde tüm değişkenlerin olduğu en geniş modelden başlanır ve değişkenler teker teker dışlanır. Bu algoritmanın uygulanabilmesi için  $n>p$ olmalıdır. 


---
# Test hatasının dolaylı tahmini 


- Bir doğrusal modelde değişken sayısın artarken SSR küçülür ve $R^2$ artar. Bu nedenle bu iki istatistik, SSR ve $R^2$ değerlendirme kıstası olamazlar. 

- Dolaylı yaklaşımda eğitim verilerinden elde edilen hata baz alınır ve aşırı uyumdan kaynaklanan sapma nedeniyle bir ceza terimi eklenir. Elde edilen bu ölçüt model karşılaştırmasında kullanılabilir. Yaygın olarak kullanılan ölçütler şunlardır: 
- Mellow's $C_p$ 
$$C_{p}=\frac{1}{n}\left(\mathrm{RSS}+2 d \hat{\sigma}^{2}\right)$$
Burad $d$ modeldeki toplam parametre sayısı ve  $\hat{\sigma}^{2}$ hata varyansının bir tahminidir. 

---
# Test hatasının dolaylı tahmini 

- Akaike's Information Criterion (AIC): 
$$\mathrm{AIC}=-2 \log L+2 \cdot d$$
Buarada $L$ logolabilirlik fonksiyonunun maksimum değeridir.  

- Bayesion Information Criterion (BIC): 
$$\mathrm{BIC}=\frac{1}{n}\left(\mathrm{RSS}+\log (n) d \hat{\sigma}^{2}\right)$$

-  $C_p$ ve $AIC$ ölçütleri doğrusal modellerde normal dağılım varsayımı altında aynıdır.  

- Test hatasının küçük olduğu modellerde bu ölçütler de küçük değerler alma eğilimindedir. Bu nedenle farklı modelleri karşılaştırırken en küçük  $C_p$, AIC, veya BIC değerine sahip olan seçilir.  

- BIC daha büyük bir ceza terimi uyguladığından AIC'ye göre daha küçük modeller seçme eğilimindedir. 

---
# Test hatasının dolaylı tahmini 

- Düzeltilmiş (Adjusted) $R^2$ 
$$\text { Adjusted } R^{2}=1-\frac{\operatorname{RSS} /(n-d-1)}{\operatorname{TSS} /(n-1)}$$
Burada RSS kalıntı kareleri toplamı ve TSS toplam kareler toplamıdır.  

- Adjusted $R^2$ model daha karmaşık hale geldikçe azalabilir ya da artabilir. Gereksiz değişkenler eklendiğinde azalır.    

- Büyük bir Düzeltilmiş $R^2$ değerine sahip bir modelin test hatası küçük olur. Bu nedenle büyük değerlere sahip modeller tercih edilir.  

---
# Test hatasının dolaysız tahmini

- Test hatası geçerleme ve çapraz-geçerleme yaklaşımlarıyla doğrudan tahmin edilebilir.  

- Bunun için veriler eğitim ve geçerleme olmak üzere iki parçaya ayrılır. Eğitim verileriyle model tahmin edilirken sadece geçerleme verileriyle test hatası tahmin edilir. 

-  $\mathcal{M}_{k}$ ile gösterilen model büyüklüğü  $k=0,1,2,\ldots$, ile endekslenmiş bir model kümesi için geçerleme ve çapraz geçerleme hatası tahmin edilir.  

- Bunların arasından en küçük test hatasına sahip olan tercih edilir.  

- Bu yöntemde test hatasının,  $\sigma^2$, tahmin edilmesi gerekmez. 

---

# Düzenlileştirme (Regularization)

* Sıradan En Küçük Kareler (OLS) yöntemi Gauss-Markov varsayımları altında sapmasız/tutarlı ve en düşük varyanslı (etkin) tahminciler verir. 

* Gözlem sayısının ( $n$ ) değişken sayısından ( $p$ ) çok daha büyük olduğu örtük olarak varsayılır: $n>> p$

* $n = p$ ise OLS tahmini **tam uyum** ile sonuçlanır. 

* $p>n$ ise sonsuz sayıda OLS çözümü vardır (sonsuz varyans). OLS kullanamayız. 

* Düzenlileştirme: model katsayılarını kısıtlayarak (shrinkage) varyansı düşürebilir miyiz?

---
# Tam Uyum: Basit Regresyon 
```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
```

.pull-left[
$n=21$, $p=1$, $R^2=0.94$
```{r, warning=FALSE, echo=FALSE} 
set.seed(123)
n <- 21 
x <- rnorm(n, mean=10, sd=2) 
u <- rnorm(n, mean=0, sd=2)
y <- 5 + 3*x + u
df1   <- tibble(id=1:n, y, x)
p1 <- df1 %>% ggplot(aes(x=x,y=y)) + 
  geom_point(size = 3) 
reg1 <- lm(y ~ x, data=df1)
p1 + geom_abline(color = "blue", aes(intercept=coef(reg1)[1], slope=coef(reg1)[2])) +
  ylim(c(20,50)) + xlim(c(6,14)) +
  theme_bw()
```
]

.pull-right[
$n=2$, $p=1$, $R^2=1$
```{r, warning=FALSE, echo=FALSE}
df2 <- df1[c(1,5),]
reg2 <- lm(y ~ x, data=df2)
 df2 %>% ggplot(aes(x=x,y=y)) + 
  geom_point(size=3) + 
   geom_abline(color = "blue", aes(intercept=coef(reg2)[1], slope=coef(reg2)[2]))+
   ylim(c(20,50)) + xlim(c(6,14)) + 
   theme_bw()
```
]

---
# Çıkıntı (Ridge) Regresyonu 

OLS amaç fonksiyonu 
$$SSR = \sum_{i=1}^n (y_i -  \beta_0 - \beta_1 x_{i1}-\ldots-\beta_p x_{ip})^2$$
Ridge regresyonu OLS'ye çok benzer ancak amaç fonksiyonuna bir ceza terimi ekler: 
$$SSR_R = \sum_{i=1}^n (y_i -  \beta_0 - \beta_1 x_{i1}-\ldots-\beta_p x_{ip})^2 + \lambda \sum_{j=1}^p \beta_j^2 = SSR + \lambda \sum_{j=1}^p \beta_j^2$$
$\lambda \geq 0$ ayarlama (tuning) parametresi

$\lambda \sum_{j=1}^p \beta_j^2$: küçültme cezası (shrinkage penalty). $\lambda = 0$ ise OLS=Ridge

$\lambda\rightarrow \infty$ ridge katsayıları, $\hat{\beta}_{\lambda}^R$, sıfıra yaklaşır. $\lambda$ değiştikçe katsayı tahminleri değişir.

---
# Örnek 
.center[![](img/creditdata.png)]
* $p=10$, Çıktı değişkeni = Balance 
* Amaç çıktı değişkenini en iyi kestiren doğrusal modeli kurmak. 
* OLS katsayıları $X$'lerin ölçü birimlerine bağlı olarak değişir. Örneğin $X=Gelir$ TL olarak ölçülmüş olsun. Eğer $Gelir2 = Gelir/1000$ dönüştürmesi ile 1000TL cinsinden yeni bir değişken yaratırsak bunun katsayısı $1000\times \hat{\beta}$ olarak değişir ve sonuçta $X\times \hat{\beta}$ aynı kalır. 
* Ridge regresyonu için ise bu özellik geçerli değildir. Bu nedenle tüm değişkenleri standardize etmek gerekir (Paydada $x_j$'nin örneklem standart sapması yer almaktadır): 
$$\tilde{x}_{i j}=\frac{x_{i j}}{\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)^{2}}}$$

---
# Örnek: Credit data 

.pull-left[
.center[![:scale 100%](img/ridge1.PNG)]
]

.pull-right[
* Bu grafik $\lambda$ değiştikçe katsayı tahminlerinin nasıl değiştiğini göstermektedir
* Dikey eksen: standardize edilmiş ridge katsayı tahminleri 
* Yatay eksen: $\lambda$ ayarlama parametresi
* $\lambda=0$: OLS katsayıları
* $\lambda$ büyüdükçe katsayılar küçülmektedir; limitte tüm katsayılar 0 olur. 
]

(ISLR Fig-6.4, p.216)

---
# Ridge regresyonunda sapma-varyans ilişkisi 
.pull-left[
.center[![:scale 100%](img/ridge2.PNG)]
]

.pull-right[
* Simülasyon verileri ile edilen grafikte $\lambda$ ile ortalama hata karesi arasındaki ilişki gösteriliyor. 
* MSE (mor) = Sapmakare (siyah) + Varyans (yeşil) + İndirgenemez hata varyansı (kesikli yatay)
* $\lambda=0$ iken sapma çok küçük ancak varyans yüksek. 
* $\lambda\approx 10$ değerine kadar MSE hızlı bir şekilde azalıyor, sapmada da bir artış var ancak çok fazla değil.
* $\lambda = 30$ için MSE en küçük.
]

(ISLR Fig-6.5, p.218)

---
# LASSO 

* Çıkıntı regresyonunun en önemli zaafı tüm değişkenlerin modelde yer almasıdır (katsayıları küçük de olsa). Model katsayıları tam olarak $\beta=0$ olmaz ( $\lambda=\infty$ değilse ). 
* Eğer amacımız değişkenlerin seçimi ise ridge regresyonu uygun olmayabilir. 
* Örneğin Credit veri setinde Balance için kurduğumuz model 10 değişkenin hepsini içerecektir. Ancak bunların içinde bazıları diğerlerinden daha önemli olabilir (income, limit, rating, student). 
* Alternatif: LASSO (Least Absolute Shrinkage and Selection Operator)
* Tıpkı Ridge regresyonu gibi LASSO regresyonu da OLS amaç fonksiyonuna bir ceza terimi ekler: 
$$\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|=\mathrm{SSR}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|$$
* LASSO'nun en önemli farkı bazı değişkenlerin katsayılarını sıfıra eşitleyerek **değişken seçimi** yapabilmesidir.

---
# Ridge ve LASSO'nun geometrik yorumu

- LASSO optimizasyon problemi aşağıdaki gibi yazılabilir: 
$$\underset{\beta}{\operatorname{minimize}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2} \quad \text { subject to } \quad \sum_{j=1}^{p}\left|\beta_{j}\right| \leq s$$

- Ridge regresyonu için optimizasyon problemi
$$\underset{\beta}{\operatorname{minimize}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2} \quad \text { subject to } \quad \sum_{j=1}^{p} \beta_{j}^{2} \leq s$$
burada $s$ kullanıcı tarafından tanımlanan ayarlanma parametresidir. 

- Bu kısıtlanmış optimizasyon problemi izleyen grafiklerdeki gibi görselleştirilebilir. Basitlik amacıyla iki parametreli bir model varsayılmıştır.  

---
# Ridge ve LASSO'nun geometrik yorumu
.pull-left[
![](img/ridge_lasso1.PNG)  
- Ridge kısıt kümesi: $\beta_{1}^2+\beta_{2}^2\leq s$
- LASSO kısıt kümesi: $\left|\beta_{1}\right|+\left|\beta_{2}\right|\leq s$
]
.pull-right[
- Eliptik çizgiler (kırmızı) kalıntı kareleri toplamı kontür çizgileridir 
- Mavi çizgiler Ridge ve LASSO kısıtlarıdır.  
- Dikkat edilirse Ridge regresyonunun kısıt kümesi parametreleri küçültürken hiçbirini sıfır yapmaz. 
- Diğer taraftan,LASSO kısıt seti bazı parametrelerin tam olarak sıfıra eşitlenmesiyle sonuçlanabilir. 
]



---
# LASSO Örnek: Credit data 

.pull-left[
.center[![:scale 100%](img/lasso1.PNG)]
]
.pull-right[
* $\lambda=0\rightarrow$ OLS
* $\lambda \rightarrow \infty$ tüm katsayılar 0 (null model)
* Ara değerler için bazı katsayılar 0. 
* Bazı değişkenler modelden dışlanıyor. 
]

(ISLR Fig-6.6, p.220)

---
# Ayarlama parametresinin seçimi 

* $\lambda$ ayarlama parametresi çapraz geçerleme (cross validation) ile seçilebilir 
* Önce $\lambda$ için bir kesikli değerler kümesi (grid) belirlenir. 
* Daha sonra her bir $\lambda_j$ değeri için çapraz geçerleme hatası hesaplanır.
* En küçük çapraz geçerleme hatasını veren $\lambda$ değeri seçilir. 
* Son olarak, seçilen $\lambda$ parametresi ile model tahmin edilir. 
![:scale 100%](img/ridge3.PNG)

---
# Elastik Net 

* [Zou ve Hastie (2005)](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2005.00503.x) ridge ve LASSO regresyonlarını özel durum olarak barındıran bir model önermiştir. 

* Naif elastik net aşağıdaki fonksiyonu en küçük yapacak şekilde katsayıları seçer: 
$$\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+ \lambda_1  \sum_{j=1}^p \beta_j^2+\lambda_2 \sum_{j=1}^{p}\left|\beta_{j}\right|=\mathrm{SSR}+\lambda_1  \sum_{j=1}^p \beta_j^2+\lambda_2 \sum_{j=1}^{p}\left|\beta_{j}\right|$$

* Naif yaklaşım: iki adımlı tahmin, önce Verilmiş bir $\lambda_2$ değeri için ridge regresyonunu tahmin et; ikinci adıma LASSO uygula. 

* Ancak bu yöntem iki kere küçültme yaptığı için kestirim performansı başarılı değildir. 

* Zou ve Hastie naif yaklaşım yerine alternatif bir tahmin çerçevesi önermiştir.


---
# Elastic Net 


- Amaç fonksiyonu aşağıdaki gibi yazılabilir: 
$$\mathrm{SSR}+\lambda \left[ (1-\alpha) \sum_{j=1}^p \beta_j^2+\alpha \sum_{j=1}^{p}\left|\beta_{j}\right| \right]$$
$0\leq \alpha \leq 1$. 
-  $\alpha=0$ olduğunda ridge regresyonu,   $\alpha=1$ olduğunda ise LASSO elde edilir.  

- Aslında elastik net bu iki arasında bir yerdedir. Bire yakın $\alpha$ değerleri için elastik net LASSO'ya benzer davranış gösterir. Ancak özellikle değişkenler arasında çok yüksek korelasyonun olduğu durumlarda LASSO'ya göre daha iyi bir performans gösterebilir.  

-  `R` kütüphanesi `glmnet` kullanıcıların $\alpha$ parametresini seçmesine izin verir. Bkz. [An Introduction to glmnet](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf)

---

# Post-double selection LASSO

- LASSO'nun model seçiminde kullanılabileceğini öğrendik. Ancak model seçiminde iktisat teorisi de önemlidir. Örneğin, teorik modelden hareketle oluşturulan aşağıdaki modeli düşünelim: 
$$y = \alpha\ d + X\beta + \epsilon$$
burada  $y$ tepki değişkeni, $d$ potansiyel olarak içsel (endogenous) değişken (treatment variable) ve $X$ ise  $p$ kontrol değişkenini gösteren matristir. ($p>>n$ olabilir). 

- Kontrol değişkenlerini, $x$, nasıl seçmeliyiz? Tek adımlı seçim prosedürü tipik olarak şöyle çalışır: herhangi bir $x_j$ değişkeni LASSO ve t-testi sonuçlarına göre anlamlıysa modele dahil edilir (post single-selection procedure).  

- Belloni, Chernozukov ve Hansen (2014), parametrelerin sıfıra yakın (ancak tam olarak sıfır değil) olduğu durumda bu yaklaşımın başarısız olacağını göstermiştir.   


---

# Değişken seçimi

- Örnek olarak Acemoglu, Johnson, ve Robinson (2001) tarafından incelenen kurum kalitesi ve ekonomik büyüme performansı ilişkisine bakalım: 

$$log(GDPpc) = \alpha~ Q + X\beta+\epsilon$$

burada $Q$ kurum kalitesini gösteren bir değişkendir. Kontrol değişkenleri arasında coğrafi ve ülkeye özgü bir çok faktör yer almaktadır.   

- Bu modelde  $Q$ içseldir. Acemoglu et al. (2001) erken dönem yerleşimcilerinin yaşam süresini araç değişken olarak kullanmıştır.  

- Ancak hangi $X$ değişkenlerinin modele eklenmesi gerektiği açık değildir. İki adımlı LASSO seçimi bu konuda yardımcı olabilir.  

---

# Double-Selection LASSO

- Belloni, Chernozhukov, ve Hansen tarafından önerilmiştir.  

- Algoritma adımları: (daha detaylı bilgi için bkz. [How to do model selection with inference in mind](https://stuff.mit.edu/~vchern/papers/Chernozhukov-Saloniki.pdf))

Adım 1: LASSO (veya başka bir düzenlileştirme yöntemini) kullanarak $y$'yi en iyi kestiren $x_j$ değişkenlerini bul. 

Adım 2: LASSO (veya başka bir düzenlileştirme yöntemini) kullanarak $d$'yi en iyi kestiren $x_j$ değişkenlerini bul. 

Adım 3: Her iki adımda seçilen değişkenleri kullanarak OLS ile modeli yeniden tahmin et 

- Dikkat edilirse değişken seçimi iki kere uygulanır: ilk adımda $y$ tepki değişkenidir, ikinci adımda ise $d$ tepki değişkenidir. Burada amaç dışlanmış değişken sapmasını en küçük yapmaktır.  


---

# Boyut küçültme yöntemleri 

- Bu yöntemler kestirim değişkenlerinin bir dönüştürmesini bularak modelin boyutu küçültür.   

-  $Z_1,Z_2,\ldots,Z_M$, $p$ kestirim değişkeninin  $M<p$ doğrusal kombinasyonu olsun:  
$$Z_{m}=\sum_{j=1}^{p} \phi_{m j} X_{j}$$
Burada $\phi_{m 1},\ldots,\phi_{m p}$ doğrusal kombinasyon katsayılarıdır. 

- $M<p$ olduğundan model sıradan en küçük kareler yöntemi ile tahmin edilebilir: 
$$y_{i}=\theta_{0}+\sum_{m=1}^{M} \theta_{m} z_{i m}+\epsilon_{i}, \quad i=1, \ldots, n$$

---
# Temel bileşenler regresyonu 

- Temel Bileşenler Regresyonu (Principal Components Regression (PCR)) yüksek boyutlu kestirim değişkenleri kümesinin bir lineer kombinasyonunu baz alır.  

- Temel Bileşenler Analizi (PCA) birbiriyle ilişkisiz doğrusal kombinasyonların bulunmasının bir yöntemini sunar. Bir veri kümesinde değişken sayısı kadar temel bileşen bulunur. 

- Bunlardan birinci temel bileşen verilerdeki değişkenliği en fazla açıklayan bileşendir. Bu bileşen bulunduktan sonra doğrusal regresyonda kullanılabilir.   
 



